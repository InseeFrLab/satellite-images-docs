
## Entraînement et évaluation <!--  Raya -->

- IOU, Loss, zone de test manuel (calculer la taille des zones de test)

Metriques et images



## Inférence 

Dans le cadre de notre projet de détection automatique des zones bâties à partir d’images satellites, il est essentiel de distinguer les phases d’entraînement du modèle et d’inférence. L’entraînement correspond à la phase exploratoire du projet, mobilisant des ressources matérielles conséquentes (notamment des GPU) et nécessitant de nombreux choix méthodologiques (prétraitement des images, ajustement des hyperparamètres, choix du modèle, etc.). Cette étape, une fois réalisée, n'est plus réellement centrale et peut être améliorée à la marge de manière indépendante. 

À l’inverse, la phase d’inférence s’apparente à une mise en production opérationnelle du modèle entraîné. Une fois les poids du modèle estimés, la génération de prédictions devient relativement peu coûteuse, notamment grâce à la possibilité d’exécuter les inférences sur CPU avec un temps de latence acceptable. L’enjeu devient alors de concevoir un dispositif fiable, reproductible et facilement mobilisable pour mettre à disposition les résultats du modèle, en particulier lorsque de nouvelles images satellites sont acquises.

Actuellement, les inférences sont réalisées sur des tuiles disjointes de taille $250 \times 250$ pixels, qui correspondent à la taille utilisée lors de l’entraînement. Chaque image satellite de $2000 \times 2000$ pixels est ainsi découpée en 64 sous-images traitées indépendamment. Cette stratégie, bien que simple, présente plusieurs inconvénients. Elle introduit des artefacts de bordure, limite la vision contextuelle du modèle, et empêche une prise en compte fluide des objets situés à la jonction de deux tuiles.

Afin d’améliorer la cohérence spatiale des prédictions, nous mettons en œuvre une stratégie d’inférence par fenêtre glissante (*sliding window*). Celle-ci consiste à réaliser plusieurs prédictions pour un même pixel, à partir de fenêtres décalées les unes par rapport aux autres. Les probabilités ainsi obtenues sont ensuite moyennées, ce qui permet de lisser les effets de bord, de renforcer la stabilité des contours et de réduire le bruit.

<!-- LOL -->
Cette inférence multiple est complétée par un pipeline de post-traitement multiclasse. Celui-ci repose d’abord sur une stratégie de repli contextuel : pour les pixels dont la probabilité maximale est inférieure à un certain seuil, la classe est réattribuée en fonction du voisinage local (via vote majoritaire ou moyenne pondérée des probabilités). Ensuite, des opérations de morphologie mathématique sont appliquées classe par classe afin de supprimer les artefacts et lisser les masques de segmentation. Enfin, un filtrage par taille minimale est mis en œuvre, avec des seuils spécifiques à chaque classe (par exemple, un bâtiment peut être plus petit qu’une zone d’eau). Ce traitement permet d’éliminer les objets aberrants ou trop petits pour être statistiquement significatifs.
<!-- jsp ou mettre ca mais on produit aussi les evolutions avec les constructions/destructions grace aux intersections d'anciennes pred -->

Afin d’industrialiser l’ensemble de la phase d’inférence, une API a été développée. Elle encapsule l’ensemble des étapes précédentes (inférence, post-traitement) et permet une exploitation simple et flexible du modèle par les différentes équipes concernées. Trois points d’entrée sont disponibles :

1. fourniture directe d’une image satellite à prédire,
2. sélection d’une zone géographique via coordonnées GPS (*bounding box*) et année d’observation,
3. saisie d’un identifiant d’îlot (entité géographique infra-communale) pour obtenir les prédictions correspondantes. <!-- + année ? -->

L’API intègre un mécanisme de cache qui évite de recalculer une prédiction déjà effectuée pour une même zone géographique et un même modèle <!-- + année ? -->. L’ensemble est déployé sur CPU et permet une chaîne de traitement entièrement automatisée, assurant une inférence rapide dès la réception de nouvelles images satellites.

Conçue comme un outil évolutif au service des équipes métiers de l’Insee, l’API est amenée à s’adapter aux besoins exprimés. Par exemple, à la suite d’un besoin identifié concernant le calcul de statistiques par îlot, une nouvelle fonctionnalité a été ajoutée pour retourner non seulement la carte de prédictions, mais aussi des indicateurs agrégés, tels que les surfaces de bâties par îlot.

Néanmoins, la diffusion de résultats via une API n’est pas toujours la modalité la plus adaptée aux usages internes. C’est pourquoi deux modes de restitution complémentaires ont été développés : d’une part, la production de fichiers Parquet contenant les résultats structurés pour exploitation statistique ; d’autre part, une application interactive cartographique, destinée aux agents de terrain. Cette application s’appuie sur un GeoServer <!-- def --> pour diffuser les images satellites, les cartes de segmentation, ainsi que les évolutions détectées (créations, destructions de bâti) <!-- les evolutions sont calculées en amont et stockées sur le geoserver au meme titre que les pred -->, facilitant ainsi le croisement avec les informations d’enquête et les validations sur le terrain.

## Mise à disposition pour les statisticiens <!-- Thomas -->

### Vers de nouveaux indicateurs statistiques ? {#sec-indic_stats}

Contrairement à d’autres applications de l’apprentissage automatique déjà déployées à l’Insee, comme les modèles de classification de textes pour la codification automatique dans des nomenclatures, les sorties brutes d’un modèle de segmentation d’images ne constituent pas, en soi, des données statistiques directement exploitables ou diffusables. Une carte de segmentation s’apparente davantage à un support intermédiaire, qui doit faire l’objet d’une interprétation, d’un traitement spatial et d’une contextualisation pour devenir une information statistique pertinente.
Comme le souligne le Mémorandum de Varsovie, ces nouvelles formes de données issues de sources non traditionnelles peuvent cependant jouer un rôle essentiel : elles peuvent soit conduire à la construction de nouveaux indicateurs, soit servir de proxies statistiques utiles pour améliorer la qualité ou la finesse d’indicateurs existants.
En ce sens, le développement d’un modèle de segmentation performant n’a de valeur, dans le contexte de la statistique publique, que s’il permet effectivement de produire de l’information utile, fiable, et intégrable dans les systèmes d’observation existants. À défaut, l’usage d’un tel outil par les statisticiens resterait discutable, car sans finalité opérationnelle claire.

À ce stade du projet, l’enjeu est donc de déterminer les indicateurs statistiques pouvant être dérivés des sorties du modèle. Cette démarche doit reposer sur une co-construction entre les équipes métiers de l’Insee, qui expriment les besoins statistiques concrets, et la DMGC<!-- t'y as pensé bg -->, appuyée par le SSP Lab, qui assure l’évaluation de la faisabilité technique, la pertinence méthodologique, et le développement des outils associés.

Plusieurs cas d’usage ont d’ores et déjà été identifiés. L’un d'eux, détaillé en @sec-use_case, concerne la construction d’indicateurs d’évolution du bâti. Étant donné que le modèle attribue une classe à chaque pixel, et que chaque pixel représente une surface de $0,25 m^2$, il est possible de calculer la surface bâtie dans une zone géographique donnée à une date donnée. En comparant ces surfaces sur plusieurs dates, on peut alors quantifier les dynamiques de construction ou de disparition de bâtiments. Croisées avec d’autres sources de données, comme le Répertoire d’immeubles Localisés (RIL), ces surfaces bâties peuvent fournir des proxies de l’évolution du parc de logements et, par extension, de la population. De telles approches ouvrent des perspectives intéressantes pour l’enrichissement des dispositifs de suivi, en particulier dans les zones où les données administratives sont lacunaires ou peu à jour comme c'est le cas dans certains DROM.

Concrètement, nous avons produit des fichiers millésimés pour chacun des DROM, retraçant l’évolution de la surface bâtie par îlot, pour chaque année disponible. Cette base de données constitue une ressource structurée et directement exploitable pour des analyses statistiques, déjà mobilisée par la DMTR pour évaluer la qualité de l'enquête cartographique. Ces premiers résultats illustrent le potentiel d’une telle approche pour renforcer la couverture, la réactivité et la granularité des indicateurs mobilisés dans les travaux de la statistique publique territorialisée.


<!-- Parler de la politique ZAN ? -->

### Une application interactive pour les agents de terrain


Les résultats obtenus et agregée dans la base d'évolution de surface bâti peuvent être utile, on l'a vu, pour de l'analyse post enquête cartographique ou en assistant la production des chiffres de recensement ou bien encore pour réaliser des études sur l'évaluation des politiques publique comme la ZAN par exemple et l'évolution de la couverture du sol. Cependant, un autre besoin nous a été rapidement remonté directement par les agents de terrain en charge de l'organisation de l'enquête cartographique. Leur besoin est d'être capable d'estimer en amont la charge que représente une zone donnée en temps d'enquête. EN effet, dans les zones ou les habitats précaires sont nombreux les évolutions de batiments peuvent être très rapide générant alors des difficultés de gestion de l'enquête. En plus, de pouvoir identifier les zones  prioritaires pour l'enquête cartographiques les agents souhaiteraient pouvoir visualiser les changements de bâtis qu'il y a eu entre l'ancienne enquête et celles qui sont en train de préparer.   
Pour toutes ces raisons nous avons décidé de développer une application web interactive de sorte à pouvoir visualiser rapidement et facilement les résultats issus de notre modèle. L'objectif était de rendre accessible le plus simplement possible au agent de terrain de l'information issus de nos résultats. 
Ainsi cette application permet de visualiser les images satellites et leur millésimes que nous possédons pour les différents DROM (Guadeloupe, Martinique, Guyane, Réunion, Mayotte). Il est donc possible de comparer très rapidement les images pour deux années données et repérer les potentiels changements. En plus de cela nous permettons l'affichage des cartes de segmentation de sorte à visualiser les prédictions du modèles à grande echelle et de manière globale. Aussi, pour optimiser les processus de détection de changement nous calculons des masques de création et de construction à partir de 2 cartes de segmentation que nous affichons également. Un enquêteur peut ainsi très rapidement repérer les zones qui ont subi le plus de changement entre deux années. 
L'application intègre également les statistiques de surfaces évoquée en @sec-indic_stats. En effet, la carte contient les délimitations des îlots et il est possible d'accèder au information de l'îlot de manière interactive en cliquant sur la carte. Un tableau récapitulatifs avec les statistiques de tous les îlots est également présent et une fonction "recherche" est disponible pour obtenir directement les statistiques d'un ilots précis et recentrer la carte sur celui ci. L'application est accessible à l'adresse suivante : [https://inseefrlab.github.io/satellite-images-webapp/](). 

L'application est totalement modulaire et se veut répondre au besoin des enquêteurs et bénéficiera de toutes les remontées qui pourront lui en être fait afin d'amléliorer son usage.


<!-- Mettre une capture de l'appli -->

 <!-- mieux préparer les opérations de terrain ou de vérifier des résultats étranges post collecte.
<!-- localiser les ilots, les zones à enqueter
Dans tous les cas l'enjeu c'est les données. Plus tot on a accès à des données le mieux c'est => partenariat avec l'IGN indispensable
 -->

![Schéma de la pipeline](../img/app-architecture.png){#fig-pipeline width=75%}