
## Entraînement et évaluation <!--  Raya -->

- IOU, Loss, zone de test manuel (calculer la taille des zones de test)

Metriques et images



## Inférence 

Dans le cadre de notre projet de détection automatique des zones bâties à partir d’images satellites, il est essentiel de distinguer les phases d’entraînement du modèle et d’inférence. L’entraînement correspond à la phase exploratoire du projet, mobilisant des ressources matérielles conséquentes (notamment des GPU) et nécessitant de nombreux choix méthodologiques (prétraitement des images, ajustement des hyperparamètres, choix du modèle, etc.). Cette étape, une fois réalisée, n'est plus réellement centrale et peut être amélioré à la marge de manière indépendante. 

À l’inverse, la phase d’inférence s’apparente à une mise en production opérationnelle du modèle entraîné. Une fois les poids du modèle estimés, la génération de prédictions devient relativement peu coûteuse, notamment grâce à la possibilité d’exécuter les inférences sur CPU avec un temps de latence acceptable. L’enjeu devient alors de concevoir un dispositif fiable, reproductible et facilement mobilisable pour mettre à disposition les résultats du modèle, en particulier lorsque de nouvelles images satellites sont acquises.

Actuellement, les inférences sont réalisées sur des tuiles disjointes de taille $250 \times 250$ pixels, qui correspondent à la taille utilisée lors de l’entraînement. Chaque image satellite de $2000 \times 2000$ pixels est ainsi découpée en 64 sous-images traitées indépendamment. Cette stratégie, bien que simple, présente plusieurs inconvénients. Elle introduit des artefacts de bordure, limite la vision contextuelle du modèle, et empêche une prise en compte fluide des objets situés à la jonction de deux tuiles.

Afin d’améliorer la cohérence spatiale des prédictions, nous mettons en œuvre une stratégie d’inférence par fenêtre glissante (*sliding window*). Celle-ci consiste à réaliser plusieurs prédictions pour un même pixel, à partir de fenêtres décalées les unes par rapport aux autres. Les probabilités ainsi obtenues sont ensuite moyennées, ce qui permet de lisser les effets de bord, de renforcer la stabilité des contours et de réduire le bruit.

Cette inférence multiple est complétée par un pipeline de post-traitement multiclasse. Celui-ci repose d’abord sur une stratégie de repli contextuel : pour les pixels dont la probabilité maximale est inférieure à un certain seuil, la classe est réattribuée en fonction du voisinage local (via vote majoritaire ou moyenne pondérée des probabilités). Ensuite, des opérations de morphologie mathématique sont appliquées classe par classe afin de supprimer les artefacts et lisser les masques de segmentation. Enfin, un filtrage par taille minimale est mis en œuvre, avec des seuils spécifiques à chaque classe (par exemple, un bâtiment peut être plus petit qu’une zone d’eau). Ce traitement permet d’éliminer les objets aberrants ou trop petits pour être statistiquement significatifs.

Afin d’industrialiser l’ensemble de la phase d’inférence, une API a été développée. Elle encapsule l’ensemble des étapes précédentes (inférence, post-traitement) et permet une exploitation simple et flexible du modèle par les différentes équipes concernées. Trois points d’entrée sont disponibles :

1. fourniture directe d’une image satellite à prédire,
2. sélection d’une zone géographique via coordonnées GPS (*bounding box*) et année d’observation,
3. saisie d’un identifiant d’îlot (entité géographique infra-communale) pour obtenir les prédictions correspondantes.

L’API intègre un mécanisme de cache qui évite de recalculer une prédiction déjà effectuée pour une même zone géographique et un même modèle. L’ensemble est déployé sur CPU et permet une chaîne de traitement entièrement automatisée, assurant une inférence rapide dès la réception de nouvelles images satellites.

Conçue comme un outil évolutif au service des équipes métiers de l’Insee, l’API est amenée à s’adapter aux besoins exprimés. Par exemple, à la suite d’un besoin identifié concernant le calcul de statistiques par îlot, une nouvelle fonctionnalité a été ajoutée pour retourner non seulement la carte de prédictions, mais aussi des indicateurs agrégés, tels que les surfaces de bâties par îlot.

Néanmoins, la diffusion de résultats via une API n’est pas toujours la modalité la plus adaptée aux usages internes. C’est pourquoi deux modes de restitution complémentaires ont été développés : d’une part, la production de fichiers Parquet contenant les résultats structurés pour exploitation statistique ; d’autre part, une application interactive cartographique, destinée aux agents de terrain. Cette application s’appuie sur un GeoServer pour diffuser les images satellites, les cartes de segmentation, ainsi que les évolutions détectées (créations, destructions de bâti), facilitant ainsi le croisement avec les informations d’enquête et les validations sur le terrain.

## Mise à disposition pour les statisticiens <!-- Thomas -->

### Vers de nouveaux indicateurs statistiques ?

Contrairement à d’autres applications de l’apprentissage automatique déjà déployées à l’Insee, comme les modèles de classification de textes pour la codification automatique dans des nomenclatures, les sorties brutes d’un modèle de segmentation d’images ne constituent pas, en soi, des données statistiques directement exploitables ou diffusables. Une carte de segmentation s’apparente davantage à un support intermédiaire, qui doit faire l’objet d’une interprétation, d’un traitement spatial et d’une contextualisation pour devenir une information statistique pertinente.
Comme le souligne le Mémorandum de Varsovie, ces nouvelles formes de données issues de sources non traditionnelles peuvent cependant jouer un rôle essentiel : elles peuvent soit conduire à la construction de nouveaux indicateurs, soit servir de proxies statistiques utiles pour améliorer la qualité ou la finesse d’indicateurs existants.
En ce sens, le développement d’un modèle de segmentation performant n’a de valeur, dans le contexte de la statistique publique, que s’il permet effectivement de produire de l’information utile, fiable, et intégrable dans les systèmes d’observation existants. À défaut, l’usage d’un tel outil par les statisticiens resterait discutable, car sans finalité opérationnelle claire.

À ce stade du projet, l’enjeu est donc de déterminer les indicateurs statistiques pouvant être dérivés des sorties du modèle. Cette démarche doit reposer sur une co-construction entre les équipes métiers de l’Insee, qui expriment les besoins statistiques concrets, et la DMGC, appuyée par le SSP Lab, qui assure l’évaluation de la faisabilité technique, la pertinence méthodologique, et le développement des outils associés.

Plusieurs cas d’usage ont d’ores et déjà été identifiés. L’un d'eux, détaillé en Section 4, concerne la construction d’indicateurs d’évolution du bâti. Étant donné que le modèle attribue une classe à chaque pixel, et que chaque pixel représente une surface de $0,25 m^2$, il est possible de calculer la surface bâtie dans une zone géographique donnée à une date donnée. En comparant ces surfaces sur plusieurs dates, on peut alors quantifier les dynamiques de construction ou de disparition de bâtiments. Croisées avec d’autres sources de données, comme le Répertoire d’Immeubles Localisés (RIL), ces surfaces bâties peuvent fournir des proxies de l’évolution du parc de logements et, par extension, de la population. De telles approches ouvrent des perspectives intéressantes pour l’enrichissement des dispositifs de suivi, en particulier dans les zones où les données administratives sont lacunaires ou peu à jour comme c'est le cas dans certains DROM.

Concrètement, nous avons produit des fichiers millésimés pour chacun des DROM, retraçant l’évolution de la surface bâtie par îlot, pour chaque année disponible. Cette base de données constitue une ressource structurée et directement exploitable pour des analyses statistiques, déjà mobilisée par la DMTR pour évaluer la qualité de l'enquête cartographique. Ces premiers résultats illustrent le potentiel d’une telle approche pour renforcer la couverture, la réactivité et la granularité des indicateurs mobilisés dans les travaux de la statistique publique territorialisée.


<!-- Parler de la politique ZAN ? -->

### Une application interactive pour les agents de terrain


- Une application web interactive a été développée pour permettre une consultation directe des résultats
- Cette application s’appuie sur un GeoServer qui héberge et diffuse les images satellites et les cartes de segmentation associées.
- Elle offre une interface cartographique permettant aux agents de l’Insee en charge des enquêtes de terrain :

  - d’accéder aux images avant/après sur une zone donnée,
  - de visualiser les détections de destructions et de créations de bâtiments,
  - et ainsi de mieux préparer les opérations de terrain ou de **vérifier des résultats étranges post collecte.


Dans tous les cas l'enjeu c'est les données. Plus tot on a accès à des données le mieux c'est => partenariat avec l'IGN indispensable


![Schéma de la pipeline](../img/app-architecture.png){#fig-pipeline width=75%}