Nous allons dans cette partie revenir sur les concepts d'apprentissage profond (*deep learning*) afin d'expliquer notre démarche et l'application de ces méthodes à nos problématiques précises. Nous définirons ensuite les données que nous avons utilisée avant de détailler les méthodes d'évaluation que nous avons appliqués à nos algorithmes.

## Les modèles de d'apprentissage profond pour l'analyse d'images

Dans le domaine de l'apprentissage statistique, les réseaux de neurones profonds en sont un sous-domaine. Si les premiers algorithmes de réseaux de neurones ont été développé dès les années, les réseaux de neurones profonds ont eux fait leur révolution dans les années 2010 grâce notamment aux développement de nouvelles cartes graphiques (GPU) et d'algorithmes optimisées pour celles ci. En effet, leur développement a longtemps été freiné par la quantité de données nécessaires pour obtenir de bons résultats impliquant donc des temps de calculs très longs et des coûts d'annotations prohibitifs. La conception des réseaux de neurones profond est inspirée du fonctionnement du cerveau humain. En effet, l'idée est de représenter les neurones par des fonctions mathématiques très simples qui s'activent si le signal reçu en entrée est suffisamment fort et transmettent alors leur "activation" dans ce cas. Un grand nombre de neurones correctement organisés permet alors de réaliser des opérations plus complexes, résultantes des différentes activations.

Si l'apprentissage profond est devenu la solution de facto pour l'analyse d'images par ordinateur, et notamment les tâches de segmentation sémantique c'est en parti grâce aux travaux fondateurs de @long2015fullyconvolutionalnetworkssemantic qui ont développé les réseaux entièrement convolutifs, une extension de l'architecture des réseaux de neurones convolutif proposé par @LeCun1989.

### Réseaux de neurones convolutifs

Les réseaux de neurones convolutifs (CNN) tirent leur inspiration du cortex visuel des animaux et se composent de deux parties (voir @fig-CNN) :

1. Une première partie composée par des couches de convolutions successives qui permet d'extraire des prédicteurs de l'image en entrée du réseau;
2. Une seconde partie permettant de classifier les pixels de l'image en entrée à partir des prédicteurs obtenus dans la première partie.

::: {}
![Réseau de neurones convolutif](../img/CNN.png){#fig-CNN width=75%}

*Note de lecture : La partie convolutive est représentée par les différents cubes à gauche et la partie classifiante est représentée par les rectangles fins et bleus à droite* 
:::

Dans la partie convolutive, l'image en entrée du réseau se voit appliquer des filtres appelés convolutions, représentés par des matrices $A = (a_{ij})$ de petite taille appelés noyaux de convolution. L'image en sortie de l'opération de convolution est obtenue à partir de chaque pixel de l'image en entrée en calculant la somme des pixels avoisinant, pondérée par les coefficients $a_{ij}$ du noyau de convolution. Cette opération de convolution est illustrée dans la @fig-ex_conv, extraite de l'ouvrage @Kim_2017.

::: {}
![Opération de convolution](../img/conv.jpg){#fig-ex_conv width=50%}

*Note de lecture : L'image résultante est une image plus petite dont les pixels sont égaux à la somme des pixels de l'image en entrée pondérée par les coefficients du noyau de convolution.* 
:::

Une convolution vise ainsi à résumer l'information présente dans une région de l’image. Il est d'ailleurs intéressant de remarquer que lorsque le voisinage d’un pixel ressemble à la structure du noyau de convolution, la valeur de sortie est élevée. Ainsi, si le filtre est, par exemple, un détecteur de ligne verticale, il générera une grande valeur sur les pixels appartenant à des lignes verticales. Un filtre de convolution permet donc d'obtenir une sorte de carte de caractéristiques de l'image. Une couche convolutive correspond finalement à l'application d'un nombre $n_f$ de filtres différents en parallèle. Ainsi, en sortie d'une couche, il y a autant d'images (carte de caractéristiques) que de filtres appliqués. Ces $n_f$ cartes deviennent les canaux d’entrée de la couche suivante.

Les CNN se composent de couches successives, chacune ayant un rôle spécifique.  Les premières couches détectent des motifs simples dans l'image (bords, textures, lignes horizontales ou verticales...) tandis que les couches intermédiaires et profondes, plus spécialisées, combinent ces motifs simples pour repérer des formes de plus en plus complexes (motifs, objets, visages…). La @fig-Convolution schématise un réseau de neurones convolutif.

::: {}
![Représentation d'une convolution](../img/convolution.png){#fig-Convolution width=75%}

*Note de lecture : Les premières couches convolutives reconnaissent les formes simples tandis que les couches les plus profondes à droite reconnaissent des formes plus concrètes.* 
:::

Après une couche de convolution, la taille des cartes de caractéristiques reste identique à celle de l'image, ce qui peut rapidement entraîner un volume de données important et rendre le modèle coûteux à entraîner. Pour pallier cela, une méthode couramment utilisée est le *max pooling*. Cette opération, représenté par la @fig-Maxpool consiste à diviser chaque carte de caractéristiques en petites régions, puis à ne conserver, pour chacune, que la valeur maximale. Le *max pooling* permet ainsi de réduire la dimension spatiale des données tout en conservant l’information la plus pertinente. Cette réduction favorise également une certaine invariance locale aux petites translations ou déformations de l’image, et contribue à limiter le nombre de paramètres et la complexité de calcul du réseau, tout en mettant en valeur les motifs les plus discriminants. D'autres opérations telles que le padding détaillé dans la @fig-Padding permettent malgré tout de contrôler la dimension de l'image en sortie.

::: {}
![Représentation du max pooling](../img/maxpool.png){#fig-Maxpool width=50%}

*Note de lecture : L'image résultante de l'opération de max-pooling est égale au maximum de chaque pixel dans chacune des zones dessinées sur l'image de gauche.* 
:::

::: {}
![Représentation du padding](../img/padding.png){#fig-Padding width=75%}

*Note de lecture : l'ajout d'une bande de 0 autour de l'image avant d'appliquer la convolution permet de limiter la réduction de dimension de l'image résultante.* 
:::


Une fonction d’activation non linéaire, comme la ReLU (Rectified Linear Unit), est appliquée systématiquement après chaque couche de convolution (cf. @fig-relu). Inspirée du fonctionnement des neurones biologiques, cette opération introduit une non-linéarité dans le réseau. Sans cette étape, l’empilement de couches convolutives ne ferait qu’appliquer une combinaison linéaire de filtres, ce qui limiterait la capacité du modèle à apprendre des relations complexes. L’ajout de fonctions d’activation non linéaires permet ainsi au réseau d’extraire et de modéliser des motifs variés et des structures non linéaires présentes dans les données, ce qui est essentiel pour traiter efficacement des images. L’utilisation de fonctions d’activation non linéaires ne se limite pas aux réseaux convolutifs, c’est justement une composante fondamentale de l’ensemble des architectures d'apprentissage profond.

::: {.center width=40%}
![Fonction d'activation ReLu](../img/relu.png){#fig-relu}

*Note de lecture : la fonction ReLU "s'active" quand l'argument est positif.* 
:::

Contrairement aux approches classiques du machine learning, la particularité des réseaux convolutifs réside dans l’automatisation de l’extraction des caractéristiques (*feature extraction*) de l’image. Dans un cadre traditionnel, cette étape est généralement réalisée de manière manuelle ou repose sur l’expertise métier : on choisit alors à l’avance quels prédicteurs utiliser (par exemple, des statistiques sur certaines bandes spectrales ou des filtres définis à la main). À l’inverse, dans un réseau de neurones convolutif, ce sont les poids des noyaux de convolution eux-mêmes qui sont appris automatiquement au cours de l’entraînement. Ces coefficients font partie du vecteur de paramètres du modèle, noté $\theta$. Ainsi, une fois le réseau entraîné, l’ensemble des filtres optimaux $\theta^{*}$ représente la meilleure façon de transformer et d’extraire les informations pertinentes de l’image. Les cartes de caractéristiques produites par l’enchaînement de ces filtres alimentent ensuite la partie classifiante du réseau. Cette approche permet au modèle de découvrir de manière autonome les motifs les plus discriminants pour la tâche visée, sans intervention manuelle dans la conception des prédicteurs.


En sortie d’un réseau de neurones convolutif classique, la partie dite classifiante permet d’associer une catégorie à l’image analysée. Par exemple, on peut vouloir déterminer si une image représente un chien ou un chat. Pour cela, la sortie de la partie convolutive (c’est-à‑dire les cartes de caractéristiques extraites par les filtres) est d’abord transformée en un seul vecteur grâce à une opération de mise à plat (*flattening*). Ce vecteur est ensuite traité par un réseau de neurones dense, appelé aussi *fully connected*, qui produit en sortie un vecteur de scores $x = (x_0, ..., x_9)$ lorsqu’on cherche à classer l’image parmi 10 classes possibles.

Pour interpréter ces scores comme des probabilités, on applique la fonction *softmax* :

$$
\text{Softmax}(x_i) = \frac{\exp{x_i}} {\sum\limits_{j=0}^{9}{\exp{x_j}}}.
$$

Cette opération convertit les scores en une distribution de probabilité, dont la somme vaut 1, et permet de sélectionner la classe associée à la probabilité la plus élevée comme prédiction finale. La fonction *softmax* est particulièrement utile car elle est infiniment dérivable, ce qui garantit la dérivabilité de tout le modèle $f_{\theta}$ par rapport à ses paramètres $\theta$. Cela rend possible l’utilisation de la descente de gradient pour ajuster les poids du réseau au cours de l’apprentissage.


### Segmentation sémantique

Comme nous l’avons vu, les réseaux de neurones convolutifs (CNN) sont particulièrement bien adaptés aux tâches de classification d’images. Historiquement, ils ont démontré leur efficacité sur des problèmes comme la reconnaissance de chiffres manuscrits (ex. : MNIST[^mnist]), où l’objectif est d’assigner une étiquette à une image dans son ensemble.
[^mnist]: [https://fr.wikipedia.org/wiki/Base_de_donn%C3%A9es_MNIST](https://fr.wikipedia.org/wiki/Base_de_donn%C3%A9es_MNIST)

Dans notre projet, la problématique est différente : il s’agit d’identifier automatiquement les zones bâties sur des images satellites. Autrement dit, on ne cherche pas à classifier une image globalement, mais à déterminer pour chaque pixel s’il appartient à une catégorie donnée (bâti ou non bâti, par exemple). Cette tâche s'inscrit dans le cadre de la segmentation sémantique, dont l’objectif est de produire une prédiction dense, c’est-à-dire une carte de labels de même dimension spatiale que l’image d’entrée.

La segmentation sémantique a connu un tournant majeur avec l’introduction des *Fully Convolutional Networks* (FCN), notamment dans l’article fondateur de @long2015fullyconvolutionalnetworkssemantic. Ce travail a proposé une adaptation des CNN à la segmentation en éliminant la nécessité d’aplatir l’image avant la classification, une étape qui, dans les CNN traditionnels, conduit à la perte de la structure spatiale de l’image (c’est-à-dire les relations entre pixels voisins), pourtant cruciale pour localiser précisément les objets.

Dans les CNN classiques, on empile des couches de convolution qui extraient des caractéristiques de plus en plus abstraites, et l’on réduit progressivement la résolution spatiale à l’aide du *max pooling*. La profondeur de l’image (le nombre de canaux) augmente, mais sa taille spatiale (hauteur × largeur) diminue. Finalement, les données sont aplaties et envoyées dans une ou plusieurs couches entièrement connectées pour produire une prédiction globale. Mais cette architecture n’est pas compatible avec une prédiction localisée pixel à pixel.

L’idée clé des FCN est de supprimer ces couches entièrement connectées et l’étape d’aplatissement, et de les remplacer par des convolutions $1 \times 1$. Cette opération permet de transformer chaque vecteur de caractéristiques (par pixel) en un score de classe, tout en conservant la structure spatiale. On obtient alors, en sortie, une carte par classe, où chaque pixel contient une prédiction de probabilité.

Un problème subsiste toutefois : à cause des opérations de *max pooling* successives, la sortie du réseau est plus petite que l’image d’entrée. Or, pour produire une prédiction par pixel à la résolution d’origine, il est nécessaire de restaurer la taille initiale. Pour cela, @long2015fullyconvolutionalnetworkssemantic proposent l’utilisation de la convolution transposée (souvent appelée déconvolution, bien que ce terme soit techniquement incorrect), qui agit comme une opération inverse de la convolution en augmentant la résolution spatiale. La @fig-deconvolution illustre schématiquement ce mécanisme.

::: {.center}
![Représentation de la déconvolution](../img/transposed_conv.png){#fig-deconvolution}

*Note de lecture : TODO.* 
:::


<!-- The transpose of convolving a 3 × 3 kernel over a 4 × 4 input using unit strides (i.e., i = 4, k = 3, s = 1 and p = 0). It is equivalent to convolving a 3 × 3 kernel over a 2 × 2 input padded with a 2 × 2 border of zeros using unit strides (i.e., i' = 2, k' = k, s'= 1 and p' = 2). 
SOURCE : https://arxiv.org/pdf/1603.07285 Page 23-->


Ainsi pour résumer, les modèles de segmentation sont généralement représentés par une structure en forme de U avec :

- Une partie descendante, l’encodeur, qui va permettre de transformer l’image en entrée en un vecteur numérique de taille réduite par rapport au nombre de pixels initial de précédente grâce à des couches successives de convolution.
- Une partie ascendante, le décodeur, qui va partir du vecteur obtenu précédemment (aussi appelé embedding) et remonter par opérations de déconvolutions à une sortie de la même dimension que l’image.
Il est important de noter que ces deux parties du U sont paramétrées. En effet, les poids des différents noyaux de convolution et de déconvolution sont appris lors de l'entraînement.

Parmi les extensions aux FCN les plus remarquables on peut citer le modèle U-net @Ronneberger_Fischer_Brox_2015b. Dans les architectures FCN classique, les couches de convolution associée au pooling réduisent progressivement la taille des cartes de caractéristiques ce qui supprime les détails spatiaux fin et génère des segmentations floues. L'idée novatrice des auteurs est de construire une architecture en U symétrique de sorte à réutiliser les cartes caractéristiques produites lors de la partie descendante dans la partie ascendante afin qu'elle bénéficie des détails locaux comme le montre la @fig-unet. D'autres architectures comme le modèle DeepLabv3 proposé par @chen2017rethinking ont également permis de minimiser la perte d'information lors de l'opération de pooling. Leur idée est d'utiliser des convolutions dilatés (*atrous convolution* ou *dilated convolution*), c'est-à-dire des convolutions de plus grande taille mais avec des zéros entre les poids du filtre. Cela permet de maintenir une quantité d'information suffisante sans augmenter le nombre de paramètre à estimer.

::: {.center}
![Représentation schématique du U-Net](../img/Unet-remanie.png){#fig-unet}

*Note de lecture : TODO*
:::

Malgré ces réelles avancées, les architectures basées sur les convolutions présentent plusieurs limites. En effet, par construction ils sont une portée très locale puisqu'une convolution n'observe qu'un voisinage local. Pour élargir le contexte il est nécessaire d'empiler des couches au coût d'un accroissement du temps de calcul. Ainsi, s'ils sont très bon localement, ils ont tendance à ne pas être invariant spatialement (i.e. une voiture en haut à gauche de l’image ou en bas à droite reste une voiture.) et deux parties d'un même objet éloignées dans l'image sont rarement reliés ensemble.

La révolution amorcée par les Transformers (voir @vaswani2017attention) dans le traitement automatique du langage naturel (NLP) a également marqué un tournant dans le domaine de la vision par ordinateur. En 2020, les Vision Transformers (ViT), introduits par @dosovitskiy2021image chercheurs de Google, ont bouleversé les approches classiques de la vision artificielle. Leur particularité réside dans l’adoption du même mécanisme fondamental que celui utilisé en NLP : le *self-attention*. Ce mécanisme permet au modèle de se concentrer sur différentes parties d’une image (comme il le ferait avec des mots dans une phrase), afin de mieux en comprendre la structure et le contenu.

Contrairement aux réseaux convolutifs (CNN) qui balayent une image via des filtres, les Vision Transformers commencent par diviser l’image en petits blocs appelés *patches*. Chaque patch est ensuite aplatit et encodé comme un vecteur (*embedding*), exactement comme un mot l’est dans un modèle de langage. Il faut donc s'imaginer qu'une image devient une séquence de vecteurs de la même manière qu'une phrase est une séquence de mots. Cependant, contrairement au texte, les *patches* d’images n’ont pas d’ordre intrinsèque donc il est nécessaire de rajouter une information sur la position de chaque patch dans l'image. C'est ce qu'on appelle l'encodage positionnel (*positional encoding*). Finalement, une fois que tous les *patches* de l'image sont encodés on leur applique le mécaniste de *self-attention* qui va permettre de lier tous les *patches* entre eux en fonction de leur importance et ainsi capturer les dépendances aussi bien locale que globale. On obtient alors de nouveaux vecteurs qui sont des représentations des *patches* individuels, enrichies par le contexte global. Il est important de noter qu'on empile généralement plusieurs couches Transformer (par exemple, "$L \times$" signifie L couches dans la @fig-vit). Finalement, les vecteurs obtenus pour chaque patch peuvent être utilisé pour réaliser une tâche de classification classique avec une dernière couche de réseaux de neurones entièrement connectés.

<!--Note:  Pour comprendre intuitivement le mecanisme d'attention, chaque split de l'image est un vecteur de grande dimension au départ. Ensuite on le compare aux vecteur de tous les autres split (patches) de l'image et on ajuste les poids du vecteur. On fait ca pour tous les splits et plusieurs fois en changeant la taille du vecteur pour chaque patch puis on utile tout les vecteurs pour de la classif ou segmentation -->

::: {.center}
![Représentation schématique du Vision Transformer (ViT)](../img/vit_schema.png){#fig-vit}

*Note de lecture : TODO*
:::

Le succès des ViT, originalement conçus plutôt pour des tâches de classification d'images a permis de nouvelles avancées pour la segmentation d'image. Les ViT classiques présentent plusieurs limites pour réaliser de la segmentation : 

1. Les informations fines de localisation sont perdues et l'encodage positionnel n'est pas assez précis pour de la segmentation
2. Contrairement aux CNN qui construisent des représentations à différentes échelles (petits détails et vue globale), le ViT standard traite tous les *patches* à la même résolution.
3. Le mécanisme d’attention a un coût quadratique avec la taille des patches. Plus on veut de détails (donc des patches plus petits), plus ça devient lourd à calculer.

Ce sont ces limites que le modèle Segformer @xie2021segformer, développé par Nvidia,  tentent de palier. Le modèle SegFormer, est défini par deux blocs clés un encodeur (hiérarchique) basé sur des couches Transformer et un décodeur très simple pour produire la carte de segmentation représenté dans la @fig-segformer. L'idée derrière le SegFormer est de faire un mix entre les bénéfices des CNN (hiérarchie et efficacité locale) et ceux des ViT (efficacité globale). Ainsi, au lieu d’un seul niveau comme dans les ViT, le SegFormer propose un encodeur en plusieurs étages, comme un CNN. L’image passe par plusieurs couches Transformer, chacune réduisant la résolution progressivement. De plus, les patchs sont construits en se chevauchant  (*Overlapped Patch Merging*) de sorte à garder une meilleure continuité spatiale. Les auteurs montrent que cette structure hiérarchique à multi-résolution ainsi que ces fenêtres locales chevauchantes permettent de se passer de l'encodage positionnel utilisé dans les ViT.
Finalement, l'une des particularité du Segformer est également son décodeur très simple qui contraste avec les décodeurs complexes de l'U-Net ou DeepLab. En effet, il prend les sorties des différents niveaux - 4 dans le papier - de l'encodeur qui représentent des résolutions différentes. Il les projette dans un espace commun, les interpole à la même taille, puis les concatène pour produire la carte de segmentation via quelques couches simples. Tout cela permet au Segformer d'être un modèle avec relativement peu de paramètres et donc très rapide à entraîner ou inférer.

::: {.center}
![Représentation schématique du Segformer](../img/segformer_architecture.png){#fig-segformer}

*Note de lecture : TODO*
:::


D'autres modèles de segmentation peuvent être considéré à l'état de l'art comme le SETR @zheng2021SETR,  Swin Transformer @liu2021swin ou encore l'impressionnant SAM 2 (Segment Anything Model) @ravi2024sam2segmentimages développé par Meta. Cependant, dans le cadre de notre projet, après avoir testé des modèles DeepLabv3, nous avons choisi d'utiliser exclusivement le modèle Segformer-B5[^segformer] avec lequel nous avons obtenus les meilleurs résultats avec des temps de calculs tout à fait raisonnable. Pour cela, nous avons utilisé le modèle pré-entrainé mis à disposition par Nvidia et nous l'avons spécialisé sur nos images satellites des DROM.

[^segformer]: https://github.com/NVlabs/SegFormer


## Données

### Images satellites

Une image satellite est une matrice à trois dimensions. Chaque élément de cette image est un **pixel**, qui correspond à une surface au sol (par exemple 10m\*10m). Le pixel contient plusieurs valeurs numériques. Ces valeurs expriment l'intensité du rayonnement solaire reflété dans chaque **bande spectrale** pour ce pixel. Une bande spectrale correspond à une portion du spectre électromagnétique, qui peut être par exemple le bleu, le rouge, le vert, le proche infrarouge. Donc, pour une image satellite de 200\*200 pixels avec les trois bandes du visible (rouge, vert, bleu), chaque pixel aura trois valeurs différentes représentant l'intensité dans chacune des bandes du visible. Ainsi, un pixel qui représente 10m\*10m au sol donnera une couleur : du violet sur l'exemple de la @fig-pixel.

![Exemple d'un pixel d'une image satellite](../img/rgb.png){#fig-pixel width=30%}

Il existe de nombreux produits satellitaires disponibles. Nous nous sommes surtout concentrés sur deux d'entre eux : **Pléiades et Sentinel2**.

![Mamoudzou, Mayotte (2024)](../img/pleiades_vs_sentinel2.png){#fig-pleiadesvssentinel width=80%}

Les images satellites **Pléiades** constituent une ressource précieuse dans notre cas d’usage. Ce sont des images à très haute résolution, spécifiquement conçues pour l’observation fine des territoires. Elles offrent trois bandes spectrales dans le visible (**rouge, vert, bleu**) auxquelles s’ajoute une quatrième bande dans le proche infrarouge (NIR), bien que cette dernière ne soit pas disponible dans nos données actuelles. Leur résolution spatiale remarquable de **0,5m** par pixel permet une détection très précise des objets au sol, ce qui est essentiel pour nos traitements d’analyse fine.

Ces images peuvent être obtenues soit via les **archives gratuites** (sous conditions d’accord), soit par acquisition à la demande, un service payant encadré par une licence Airbus©, avec des délais d’environ 6 à 8 mois par département.

Dans le cadre du cyclone Chido qui a frappé Mayotte en décembre 2024, un plan d’urgence a permis l’accès gratuit à une mosaïque d’images Pléiades post-cyclone couvrant l’île. Une **mosaïque** est une image composite constituée d’assemblages de prises de vues réalisées à différentes dates. Elle permet d’obtenir une couverture complète, avec un minimum de zones nuageuses et une meilleure homogénéité visuelle. Ce travail de reconstitution, mené par **l'Institut National de l'information géographique et forestière** (IGN), est crucial pour garantir des données exploitables, notamment dans le cadre de l’entraînement de modèles d’analyse automatique.

Mais Pléiades n’est pas notre seule source d’imagerie satellite. Les satellites **Sentinel-2**, développés par l’Agence spatiale européenne (ESA) dans le cadre du programme Copernicus, offrent une alternative open source, particulièrement intéressante pour les analyses à large échelle ou à forte fréquence temporelle. Contrairement à Pléiades, Sentinel-2 capte **treize bandes spectrales**, réparties entre le visible, le proche infrarouge (NIR) et l’infrarouge à ondes courtes (SWIR), ce qui ouvre la voie à une multitude d’indices et d’analyses thématiques (comme la détection de la végétation, de l’humidité, ou des matériaux).

En revanche, la résolution spatiale de Sentinel-2 est moindre : selon les bandes, elle varie entre **10 m**, 20 m et 60 m, ce qui rend l'observation de petits objets ou de détails fins plus difficile. Néanmoins, ces images ont l’avantage d’être acquises automatiquement **tous les cinq jours**, garantissant une fréquence de revisite élevée et une mise à disposition régulière des données, y compris en période de crise.

Ainsi, les images Pléiades semblent être les plus adaptés pour de la détection de bâtiment dans les DROM.

- definition
- produits
- acquisition/livraison (partenariat IGN a promouvoir)


### Annotations

- Rappeler que c'est le plus important pour avoir de bons modèles
- Nous n'avons rien annoté manuellement => full automatique
- Rappel du coût d'annotation pour en avoir de qualité (en ETP ?)
- RIL (Insee), 
- BDTOPO (better)
- CoSIA 

peut etre rappeler la difficulté d'avoir des couples qui sont iso temporel