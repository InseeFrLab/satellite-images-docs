Nous allons dans cette partie revenir sur les concepts d'apprentissage profond (*deep learning*) afin d'expliquer notre démarche et l'application de ces méthodes à nos problématiques précises. Nous définirons ensuite les données que nous avons utilisée avant de détailler les méthodes d'évaluation que nous avons appliqués à nos algorithmes.

## Les modèles de d'apprentissage profond pour l'analyse d'images

Dans le domaine de l'apprentissage statistique, les réseaux de neurones profonds en sont un sous-domaine. Si les premiers algorithmes de réseaux de neurones ont été développé dès les années, les réseaux de neurones profonds ont eux fait leur révolution dans les années 2010 grâce notamment aux développement de nouvelles cartes graphiques (GPU) et d'algorithmes optimisées pour celles ci. En effet, leur développement a longtemps été freiné par la quantité de données nécessaires pour obtenir de bons résultats impliquant donc des temps de calculs très longs et des coûts d'annotations prohibitifs. La conception des réseaux de neurones profond est inspirée du fonctionnement du cerveau humain. En effet, l'idée est de représenter les neurones par des fonctions mathématiques très simples qui s'activent si le signal reçu en entrée est suffisamment fort et transmettent alors leur "activation" dans ce cas. Un grand nombre de neurones correctement organisés permet alors de réaliser des opérations plus complexes, résultantes des différentes activations.

Si l'apprentissage profond est devenu la solution de facto pour l'analyse d'images par ordinateur, et notamment les tâches de segmentation sémantique c'est en parti grâce aux travaux fondateurs de @long2015fullyconvolutionalnetworkssemantic qui ont développé les réseaux entièrement convolutifs, une extension de l'architecture des réseaux de neurones convolutif proposé par @LeCun1989.

### Réseaux de neurones convolutifs

Les réseaux de neurones convolutifs (CNN) tirent leur inspiration du cortex visuel des animaux et se composent de deux parties (voir @fig-CNN) :

1. Une première partie composée par des couches de convolutions successives qui permet d'extraire des prédicteurs de l'image en entrée du réseau;
2. Une seconde partie permettant de classifier les pixels de l'image en entrée à partir des prédicteurs obtenus dans la première partie.

::: {}
![Réseau de neurones convolutif](../img/CNN.png){#fig-CNN width=75%}

*Note de lecture : La partie convolutive est représentée par les différents cubes à gauche et la partie classifiante est représentée par les rectangles fins et bleus à droite* 
:::

Dans la partie convolutive, l'image en entrée du réseau se voit appliquer des filtres appelés convolutions, représentés par des matrices $A = (a_{ij})$ de petite taille appelés noyaux de convolution. L'image en sortie de l'opération de convolution est obtenue à partir de chaque pixel de l'image en entrée en calculant la somme des pixels avoisinant, pondérée par les coefficients $a_{ij}$ du noyau de convolution. Cette opération de convolution est illustrée dans la @fig-ex_conv, extraite de l'ouvrage @Kim_2017.

::: {}
![Opération de convolution](../img/conv.jpg){#fig-ex_conv width=50%}

*Note de lecture : L'image résultante est une image plus petite dont les pixels sont égaux à la somme des pixels de l'image en entrée pondérée par les coefficients du noyau de convolution.* 
:::

Une convolution vise ainsi à résumer l'information présente dans une région de l’image. Il est d'ailleurs intéressant de remarquer que lorsque le voisinage d’un pixel ressemble à la structure du noyau de convolution, la valeur de sortie est élevée. Ainsi, si le filtre est, par exemple, un détecteur de ligne verticale, il générera une grande valeur sur les pixels appartenant à des lignes verticales. Un filtre de convolution permet donc d'obtenir une sorte de carte de caractéristiques de l'image. Une couche convolutive correspond finalement à l'application d'un nombre $n_f$ de filtres différents en parallèle. Ainsi, en sortie d'une couche, il y a autant d'images (carte de caractéristiques) que de filtres appliqués. Ces $n_f$ cartes deviennent les canaux d’entrée de la couche suivante.

Les CNN se composent de couches successives, chacune ayant un rôle spécifique.  Les premières couches détectent des motifs simples dans l'image (bords, textures, lignes horizontales ou verticales...) tandis que les couches intermédiaires et profondes, plus spécialisées, combinent ces motifs simples pour repérer des formes de plus en plus complexes (motifs, objets, visages…). La @fig-Convolution schématise un réseau de neurones convolutif.

::: {}
![Représentation d'une convolution](../img/convolution.png){#fig-Convolution width=75%}

*Note de lecture : Les premières couches convolutives reconnaissent les formes simples tandis que les couches les plus profondes à droite reconnaissent des formes plus concrètes.* 
:::

Après une couche de convolution, la taille des cartes de caractéristiques reste identique à celle de l'image, ce qui peut rapidement entraîner un volume de données important et rendre le modèle coûteux à entraîner. Pour pallier cela, une méthode couramment utilisée est le *max pooling*. Cette opération, représenté par la @fig-Maxpool consiste à diviser chaque carte de caractéristiques en petites régions, puis à ne conserver, pour chacune, que la valeur maximale. Le *max pooling* permet ainsi de réduire la dimension spatiale des données tout en conservant l’information la plus pertinente. Cette réduction favorise également une certaine invariance locale aux petites translations ou déformations de l’image, et contribue à limiter le nombre de paramètres et la complexité de calcul du réseau, tout en mettant en valeur les motifs les plus discriminants. D'autres opérations telles que le padding détaillé dans la @fig-Padding permettent malgré tout de contrôler la dimension de l'image en sortie.

::: {}
![Représentation du max pooling](../img/maxpool.png){#fig-Maxpool width=50%}

*Note de lecture : L'image résultante de l'opération de max-pooling est égale au maximum de chaque pixel dans chacune des zones dessinées sur l'image de gauche.* 
:::

::: {}
![Représentation du padding](../img/padding.png){#fig-Padding width=75%}

*Note de lecture : l'ajout d'une bande de 0 autour de l'image avant d'appliquer la convolution permet de limiter la réduction de dimension de l'image résultante.* 
:::


Une fonction d’activation non linéaire, comme la ReLU (Rectified Linear Unit), est appliquée systématiquement après chaque couche de convolution (cf. @fig-relu). Inspirée du fonctionnement des neurones biologiques, cette opération introduit une non-linéarité dans le réseau. Sans cette étape, l’empilement de couches convolutives ne ferait qu’appliquer une combinaison linéaire de filtres, ce qui limiterait la capacité du modèle à apprendre des relations complexes. L’ajout de fonctions d’activation non linéaires permet ainsi au réseau d’extraire et de modéliser des motifs variés et des structures non linéaires présentes dans les données, ce qui est essentiel pour traiter efficacement des images. L’utilisation de fonctions d’activation non linéaires ne se limite pas aux réseaux convolutifs, c’est justement une composante fondamentale de l’ensemble des architectures d'apprentissage profond.

::: {.center}
![Fonction d'activation ReLu](../img/relu.png){#fig-relu width=40%}

*Note de lecture : la fonction ReLU "s'active" quand l'argument est positif.* 
:::

Contrairement aux approches classiques du machine learning, la particularité des réseaux convolutifs réside dans l’automatisation de l’extraction des caractéristiques (*feature extraction*) de l’image. Dans un cadre traditionnel, cette étape est généralement réalisée de manière manuelle ou repose sur l’expertise métier : on choisit alors à l’avance quels prédicteurs utiliser (par exemple, des statistiques sur certaines bandes spectrales ou des filtres définis à la main). À l’inverse, dans un réseau de neurones convolutif, ce sont les poids des noyaux de convolution eux-mêmes qui sont appris automatiquement au cours de l’entraînement. Ces coefficients font partie du vecteur de paramètres du modèle, noté $\theta$. Ainsi, une fois le réseau entraîné, l’ensemble des filtres optimaux $\theta^{*}$ représente la meilleure façon de transformer et d’extraire les informations pertinentes de l’image. Les cartes de caractéristiques produites par l’enchaînement de ces filtres alimentent ensuite la partie classifiante du réseau. Cette approche permet au modèle de découvrir de manière autonome les motifs les plus discriminants pour la tâche visée, sans intervention manuelle dans la conception des prédicteurs.


En sortie d’un réseau de neurones convolutif classique, la partie dite classifiante permet d’associer une catégorie à l’image analysée. Par exemple, on peut vouloir déterminer si une image représente un chien ou un chat. Pour cela, la sortie de la partie convolutive (c’est-à‑dire les cartes de caractéristiques extraites par les filtres) est d’abord transformée en un seul vecteur grâce à une opération de mise à plat (*flattening*). Ce vecteur est ensuite traité par un réseau de neurones dense, appelé aussi *fully connected*, qui produit en sortie un vecteur de scores $x = (x_0, ..., x_9)$ lorsqu’on cherche à classer l’image parmi 10 classes possibles.

Pour interpréter ces scores comme des probabilités, on applique la fonction *softmax* :

$$
\text{Softmax}(x_i) = \frac{\exp{x_i}} {\sum\limits_{j=0}^{9}{\exp{x_j}}}.
$$

Cette opération convertit les scores en une distribution de probabilité, dont la somme vaut 1, et permet de sélectionner la classe associée à la probabilité la plus élevée comme prédiction finale. La fonction *softmax* est particulièrement utile car elle est infiniment dérivable, ce qui garantit la dérivabilité de tout le modèle $f_{\theta}$ par rapport à ses paramètres $\theta$. Cela rend possible l’utilisation de la descente de gradient pour ajuster les poids du réseau au cours de l’apprentissage.


### Segmentation sémantique

1 Parler de notre objectif
2 Presenter le seminal paper des reseaux convolutif vers la segmentation semantique
3 Présenter les nouveaux types de modèles basé sur Segformer (ViT)
4 Transition vers le fine tuning et donc des données que nous possédons

On vient de le voir, les réseaux de neurones convolutifs sont particulièrement adaptés pour réaliser une prédiction par image. C'est pour cela qu'ils ont été historiquement utilisé pour accomplir de la classification d'image (par exemple, classer des écritures manuscrites de chiffre). Dans le cadre de notre projet, l'objectif est plutôt d'être capable d'identifier automatiquement des zones de bâtis sur des images satellites. L'idée n'est donc pas de faire une classification à l'échelle de l'image entière mais à l'échelle du pixel. De sorte à ce qu'il soit possible de délimiter des zones appartenant à une même classe pré-définie au sein même d'une image. C'est justement le principe de la segmentation sémantique qui retourne donc une sortie dense, c'est à dire des labels de même taille que l'image en entrée. Cette tâche a réellement fait de gros progrès suite au papier fondateur de @long2015fullyconvolutionalnetworkssemantic sur les réseaux de neurones entièrement convolutifs. 

Pour rappel, avec les réseaux de neurones convolutifs classiques on empile des couches de convolution et afin d'accroître la profondeur de l'image et grâce au mécanisme de *max pooling* on réduit sa taille. Au final, on aplatit tout afin de recréer un réseau de neurones dense *classique* permettant de réaliser une classification. Cependant cette étape d'aplatissement fait perdre complètement la structure spatiale de l'image (les liens entre pixels voisins), qui est absolument nécessaire pour faire de la segmentation sémantique, à savoir "à quel classe appartient chaque pixel ?". L'idée derrière les FCN est donc de supprimer cet aplatissement ainsi que les couches entièrement connectées pour les remplacer par une convolution 1*1. De cette manière, chaque pixel devient un classifieur local qui prédit sa classe en fonction de ce que les couches profondes ont extrait, on a alors autant de petites images que de classes dans lesquelles on souhaite segmenter.

Le problème est qu'après cette convolution 1*1, et suite à toutes les couches de *max pooling*, l'image en sortie est de plus petite taille que l'image en entrée, or pour prédire un label par pixel, l'image en sortie doit être de même taille que l'image en entrée. @long2015fullyconvolutionalnetworkssemantic propose alors une solution : la déconvolution (ou *transposed convolution*). Bien qu'il s'agissent d'un abus de langage, on peut considérer la déconvolution comme le processus inverse de la convolution dans le sens ou elle augmente la résolution. La @fig-deconvolution représente schématiquement ce mécanisme.

Ainsi, les modèles de segmentation sont généralement représenté par une structure en forme de U avec :

- Une partie descendante, l’encodeur, qui va permettre de transformer l’image en entrée en un vecteur numérique de taille réduite par rapport au nombre de pixels initial de précédente grâce à des couches successives de convolution.
- Une partie ascendante, le décodeur, qui va partir du vecteur obtenu précédemment (aussi appelé embedding) et remonter par opérations de déconvolutions à une sortie de la même dimension que l’image.
Il est important de noter que ces deux parties du U sont paramétrées. En effet, les poids des différents noyaux de convolution et de déconvolution sont appris lors de l'entraînement.

Parmi les extensions aux FCN les plus remarquables ont peu citer le modèle U-net @unet. Dans les architectures FCN classique, les couches de convolution associée au pooling réduisent progressivement la taille des cartes de caractéristiques ce qui supprime les détails spatiaux fin et génère des segmentations floues. L'idée novatrice des auteurs est de construire une architecture en U symétrique de sorte à réutiliser les cartes caractéristiques produites lors de la partie descendante dans la partie ascendante afin qu'elle bénéficie des détails locaux

Dans ce contexte on a choisi Segformer. Rappeler qu'on utilise des modèles pré-entrainer qu'on finetune simplement avec les données qu'on a a notre dispo (voir partie suivante)

De la segmentation sémantique à la detection de changement => approche temporelle 
Mentionner les modèles de change detection et dire les différences avec notre approche -->


## Données

### Images satellites

Une image satellite est une matrice à trois dimensions. Chaque élément de cette image est un **pixel**, qui correspond à une surface au sol (par exemple 10m\*10m). Le pixel contient plusieurs valeurs numériques. Ces valeurs expriment l'intensité du rayonnement solaire reflété dans chaque **bande spectrale** pour ce pixel. Une bande spectrale correspond à une portion du spectre électromagnétique, qui peut être par exemple le bleu, le rouge, le vert, le proche infrarouge. Donc, pour une image satellite de 200\*200 pixels avec les trois bandes du visible (rouge, vert, bleu), chaque pixel aura trois valeurs différentes représentant l'intensité dans chacune des bandes du visible. Ainsi, un pixel qui représente 10m\*10m au sol donnera une couleur : du violet sur l'exemple de la @fig-pixel.

![Exemple d'un pixel d'une image satellite](../img/rgb.png){#fig-pixel width=30%}

Il existe de nombreux produits satellitaires disponibles. Nous nous sommes surtout concentrés sur deux d'entre eux : **Pléiades et Sentinel2**.

![Mamoudzou, Mayotte (2024)](../img/pleiades_vs_sentinel2.png){#fig-pleiadesvssentinel width=80%}

Les images satellites **Pléiades** constituent une ressource précieuse dans notre cas d’usage. Ce sont des images à très haute résolution, spécifiquement conçues pour l’observation fine des territoires. Elles offrent trois bandes spectrales dans le visible (**rouge, vert, bleu**) auxquelles s’ajoute une quatrième bande dans le proche infrarouge (NIR), bien que cette dernière ne soit pas disponible dans nos données actuelles. Leur résolution spatiale remarquable de **0,5m** par pixel permet une détection très précise des objets au sol, ce qui est essentiel pour nos traitements d’analyse fine.

Ces images peuvent être obtenues soit via les **archives gratuites** (sous conditions d’accord), soit par acquisition à la demande, un service payant encadré par une licence Airbus©, avec des délais d’environ 6 à 8 mois par département.

Dans le cadre du cyclone Chido qui a frappé Mayotte en décembre 2024, un plan d’urgence a permis l’accès gratuit à une mosaïque d’images Pléiades post-cyclone couvrant l’île. Une **mosaïque** est une image composite constituée d’assemblages de prises de vues réalisées à différentes dates. Elle permet d’obtenir une couverture complète, avec un minimum de zones nuageuses et une meilleure homogénéité visuelle. Ce travail de reconstitution, mené par **l'Institut National de l'information géographique et forestière** (IGN), est crucial pour garantir des données exploitables, notamment dans le cadre de l’entraînement de modèles d’analyse automatique.

Mais Pléiades n’est pas notre seule source d’imagerie satellite. Les satellites **Sentinel-2**, développés par l’Agence spatiale européenne (ESA) dans le cadre du programme Copernicus, offrent une alternative open source, particulièrement intéressante pour les analyses à large échelle ou à forte fréquence temporelle. Contrairement à Pléiades, Sentinel-2 capte **treize bandes spectrales**, réparties entre le visible, le proche infrarouge (NIR) et l’infrarouge à ondes courtes (SWIR), ce qui ouvre la voie à une multitude d’indices et d’analyses thématiques (comme la détection de la végétation, de l’humidité, ou des matériaux).

En revanche, la résolution spatiale de Sentinel-2 est moindre : selon les bandes, elle varie entre **10 m**, 20 m et 60 m, ce qui rend l'observation de petits objets ou de détails fins plus difficile. Néanmoins, ces images ont l’avantage d’être acquises automatiquement **tous les cinq jours**, garantissant une fréquence de revisite élevée et une mise à disposition régulière des données, y compris en période de crise.

Ainsi, les images Pléiades semblent être les plus adaptés pour de la détection de bâtiment dans les DROM.

- definition
- produits
- acquisition/livraison (partenariat IGN a promouvoir)


### Annotations

- Rappeler que c'est le plus important pour avoir de bons modèles
- Nous n'avons rien annoté manuellement => full automatique
- Rappel du coût d'annotation pour en avoir de qualité (en ETP ?)
- RIL (Insee), 
- BDTOPO (better)
- CoSIA 

peut etre rappeler la difficulté d'avoir des couples qui sont iso temporel

##  Evaluation <!--  Raya -->

- IOU, Loss, zone de test manuel (calculer la taille des zones de test)

## Inférence et mise à disposition des résultats

Une fois le modèle entraîné et validé, l'étape d'inférence consiste à appliquer ce modèle sur de nouvelles images satellites pour prédire la présence de bâtiments. Afin de lisser les prédictions et réduire le bruit, nous avons utilisé une moyenne mobile sur les sorties du modèle. Cette technique permet d'améliorer la cohérence spatiale des résultats, en atténuant les fausses détections isolées et en renforçant les zones de prédiction continue.

Pour faciliter l'accès aux résultats, nous avons mis en place une API dédiée. Cette API permet d'interroger le modèle à la demande, en soumettant de nouvelles images satellites et en récupérant les cartes de segmentation correspondantes. Cette approche garantit une intégration aisée dans des chaînes de traitement existantes et permet une exploitation rapide des prédictions, que ce soit pour des analyses ponctuelles ou pour un suivi opérationnel à grande échelle.