Nous allons dans cette partie revenir sur les concepts d'apprentissage profond (*deep learning*) afin d'expliquer notre démarche et l'application de ces méthodes à nos problématiques précises. Nous définirons ensuite les données que nous avons utilisées pour appliquer notre méthodologie.

## Les modèles d'apprentissage profond pour l'analyse d'images

Dans le domaine de l'apprentissage statistique, les réseaux de neurones profonds en sont un sous-domaine. Si les premiers algorithmes de réseaux de neurones ont été développés dès les années 50, les réseaux de neurones profonds ont eux fait leur révolution dans les années 2010 notamment grâce aux développement de nouvelles cartes graphiques (GPU) et d'algorithmes optimisés pour ceux-ci. En effet, leur développement a longtemps été freiné par la quantité de données nécessaires pour obtenir de bons résultats impliquant donc des temps de calculs très longs et des coûts d'annotations prohibitifs. La conception des réseaux de neurones profond est inspirée du fonctionnement du cerveau humain. En effet, l'idée est de représenter les neurones par des fonctions mathématiques très simples qui s'activent si le signal reçu en entrée est suffisamment fort, dans quel cas ils transmettent alors leur "activation". Un grand nombre de neurones correctement organisés permet alors de réaliser des opérations plus complexes, résultantes des différentes activations.

Si l'apprentissage profond est devenu la solution de facto pour l'analyse d'images par ordinateur, et lus précisement les tâches de segmentation sémantique, c'est en parti grâce aux travaux fondateurs de @long2015fullyconvolutionalnetworkssemantic qui ont développé les réseaux entièrement convolutifs, une extension de l'architecture des réseaux de neurones convolutif proposé par @LeCun1989.

### Réseaux de neurones convolutifs

Les réseaux de neurones convolutifs (CNN) tirent leur inspiration du cortex visuel des animaux et se composent de deux parties (voir @fig-CNN) :

1. Une première partie composée par des couches de convolutions successives qui permet d'extraire des prédicteurs de l'image en entrée du réseau;
2. Une seconde partie permettant de classifier les pixels de l'image en entrée à partir des prédicteurs obtenus dans la première partie.

::: {}
![Réseau de neurones convolutif](../img/CNN.png){#fig-CNN width=75%}

*Note de lecture : La partie convolutive est représentée par les différents cubes à gauche et la partie classifiante par les rectangles fins et bleus à droite* 
:::

Dans la partie convolutive, l'image en entrée du réseau se voit appliquer des filtres appelés convolutions, représentés par des matrices $A = (a_{ij})$ de petite taille appelés noyaux de convolution. L'image en sortie de l'opération de convolution est obtenue à partir de chaque pixel de l'image en entrée en calculant la somme des pixels avoisinant, pondérée par les coefficients $a_{ij}$ du noyau de convolution. Cette opération de convolution est illustrée dans la @fig-ex_conv, extraite de l'ouvrage @Kim_2017.

::: {}
![Opération de convolution](../img/conv.jpg){#fig-ex_conv width=50%}

*Note de lecture : L'image résultante est une image plus petite dont les pixels sont égaux à la somme des pixels de l'image en entrée pondérée par les coefficients du noyau de convolution.* 
:::

Une convolution vise ainsi à résumer l'information présente dans une région de l’image. Il est d'ailleurs intéressant de remarquer que lorsque le voisinage d’un pixel ressemble à la structure du noyau de convolution, la valeur de sortie est élevée. Ainsi, si le filtre est, par exemple, un détecteur de lignes verticales, il générera une grande valeur sur les pixels appartenant à des lignes verticales. Un filtre de convolution permet donc d'obtenir une sorte de carte de caractéristiques de l'image. Une couche convolutive correspond finalement à l'application d'un nombre $n_f$ de filtres différents en parallèle. Ainsi, en sortie d'une couche, il y a autant d'images (carte de caractéristiques) que de filtres appliqués. Ces $n_f$ cartes deviennent les canaux d’entrée de la couche suivante.

Les CNN se composent de couches successives, chacune ayant un rôle spécifique.  Les premières couches détectent des motifs simples dans l'image (bords, textures, lignes horizontales ou verticales...) tandis que les couches intermédiaires et profondes, plus spécialisées, combinent ces motifs simples pour repérer des formes de plus en plus complexes (motifs, objets, visages…). La @fig-Convolution schématise un réseau de neurones convolutif.

::: {}
![Représentation d'une convolution](../img/convolution.png){#fig-Convolution width=75%}

*Note de lecture : Les premières couches convolutives reconnaissent les formes simples tandis que les couches les plus profondes à droite reconnaissent des formes plus concrètes.* 
:::

Après une couche de convolution, la taille des cartes de caractéristiques reste identique à celle de l'image, ce qui peut rapidement entraîner un volume de données important et rendre le modèle coûteux à entraîner. Pour pallier cela, une méthode couramment utilisée est le *pooling*. Cette opération, représentée par la @fig-Maxpool, consiste à diviser chaque carte de caractéristiques en petites régions, puis à ne conserver, pour chacune, que la valeur maximale. Lorsqu'il s'agit de prendre la valeur maximale, on parle de *max pooling* mais d'autres opérations peuvent être utilisées comme le minimum, la moyenne ou la médiane. Le *pooling* permet ainsi de réduire la dimension spatiale des données tout en conservant l’information la plus pertinente. Cette réduction favorise également une certaine invariance locale aux petites translations ou déformations de l’image, et contribue à limiter le nombre de paramètres et la complexité de calcul du réseau, tout en mettant en valeur les motifs les plus discriminants. D'autres opérations telles que le padding détaillé dans la @fig-Padding permettent malgré tout de contrôler la dimension de l'image en sortie.

::: {}
![Représentation du max pooling](../img/maxpool.png){#fig-Maxpool width=50%}

*Note de lecture : L'image résultante de l'opération de max-pooling est égale au maximum des pixels dans chacune des zones dessinées sur l'image de gauche.* 
:::

::: {}
![Représentation du padding](../img/padding.png){#fig-Padding width=75%}

*Note de lecture : l'ajout d'une bande de 0 autour de l'image avant d'appliquer la convolution permet de limiter la réduction de dimension de l'image résultante.* 
:::


Une fonction d’activation non linéaire, comme la ReLU (Rectified Linear Unit), est appliquée systématiquement après chaque couche de convolution (cf. @fig-relu). Inspirée du fonctionnement des neurones biologiques, cette opération introduit une non-linéarité dans le réseau. Sans cette étape, l’empilement de couches convolutives ne ferait qu’appliquer une combinaison linéaire de filtres, ce qui limiterait la capacité du modèle à apprendre des relations complexes. L’ajout de fonctions d’activation non linéaires permet ainsi au réseau d’extraire et de modéliser des motifs variés et des structures non linéaires présentes dans les données, ce qui est essentiel pour traiter efficacement des images. L’utilisation de fonctions d’activation non linéaires ne se limite pas aux réseaux convolutifs, c’est justement une composante fondamentale de l’ensemble des architectures d'apprentissage profond.

::: {.center width=40%}
![Fonction d'activation ReLu](../img/relu.png){#fig-relu}

*Note de lecture : la fonction ReLU "s'active" quand l'argument est positif.* 
:::

Contrairement aux approches classiques du machine learning, la particularité des réseaux convolutifs réside dans l’automatisation de l’extraction des caractéristiques (*feature extraction*) de l’image. Dans un cadre traditionnel, cette étape est généralement réalisée de manière manuelle ou repose sur l’expertise métier : on choisit alors à l’avance quels prédicteurs utiliser (par exemple, des statistiques sur certaines bandes spectrales <!-- ce terme arrive trop tot, on a pas encore défini une image, encore moins une image satellite --> ou des filtres définis à la main). À l’inverse, dans un réseau de neurones convolutif, ce sont les poids des noyaux de convolution eux-mêmes qui sont appris automatiquement au cours de l’entraînement. Ces coefficients font partie du vecteur de paramètres du modèle, noté $\theta$. Ainsi, une fois le réseau entraîné, l’ensemble des filtres optimaux $\theta^{*}$ représente la meilleure façon de transformer et d’extraire les informations pertinentes de l’image. Les cartes de caractéristiques produites par l’enchaînement de ces filtres alimentent ensuite la partie classifiante du réseau. Cette approche permet au modèle de découvrir de manière autonome les motifs les plus discriminants pour la tâche visée, sans intervention manuelle dans la conception des prédicteurs.


En sortie d’un réseau de neurones convolutif classique, la partie dite classifiante permet d’associer une catégorie à l’image analysée. Par exemple, on peut vouloir déterminer si une image représente un chien ou un chat. Pour cela, la sortie de la partie convolutive (c’est-à‑dire les cartes de caractéristiques extraites par les filtres) est d’abord transformée en un seul vecteur grâce à une opération de mise à plat (*flattening*). Ce vecteur est ensuite traité par un réseau de neurones dense, appelé aussi *fully connected*, qui produit en sortie un vecteur de scores $x = (x_0, ..., x_n)$ <!-- bizarre de parler de 10 classes quand l'exemple d'avant est chien chat, j'aurais mis "n" -->lorsqu’on cherche à classer l’image parmi n <!-- n (pareil dans la formule mathematique) -->classes possibles.

Pour interpréter ces scores comme des probabilités, on applique la fonction *softmax* :

$$
\text{Softmax}(x_i) = \frac{\exp{x_i}} {\sum\limits_{j=0}^{n}{\exp{x_j}}}.
$$

Cette opération convertit les scores en une distribution de probabilité, dont la somme vaut 1, et permet de sélectionner la classe associée à la probabilité la plus élevée comme prédiction finale. La fonction *softmax* est particulièrement utile car elle est infiniment dérivable, ce qui garantit la dérivabilité de tout le modèle $f_{\theta}$ par rapport à ses paramètres $\theta$. Cela rend possible l’utilisation de la descente de gradient pour ajuster les poids du réseau au cours de l’apprentissage.


### Segmentation sémantique

Comme nous l’avons vu, les réseaux de neurones convolutifs (CNN) sont particulièrement bien adaptés aux tâches de classification d’images. Historiquement, ils ont démontré leur efficacité sur des problèmes comme la reconnaissance de chiffres manuscrits (ex. : MNIST[^mnist]), où l’objectif est d’assigner une étiquette à une image dans son ensemble.

[^mnist]: [https://fr.wikipedia.org/wiki/Base_de_données_MNIST](https://fr.wikipedia.org/wiki/Base_de_donn%C3%A9es_MNIST)

Dans notre projet, la problématique est différente : il s’agit d’identifier automatiquement les zones bâties sur des images satellites. Autrement dit, on ne cherche pas à classifier une image globalement, mais à déterminer pour chaque pixel s’il appartient à une catégorie donnée (bâti ou non bâti, par exemple). Cette tâche s'inscrit dans le cadre de la segmentation sémantique, dont l’objectif est de produire une prédiction dense, c’est-à-dire une carte de labels de même dimension que l’image d’entrée.

La segmentation sémantique a connu un tournant majeur avec l’introduction des *Fully Convolutional Networks* (FCN), notamment dans l’article fondateur de @long2015fullyconvolutionalnetworkssemantic. Ce travail a proposé une adaptation des CNN à la segmentation en éliminant la nécessité d’aplatir l’image avant la classification, une étape qui, dans les CNN traditionnels, conduit à la perte de la structure spatiale de l’image (c’est-à-dire les relations entre pixels voisins), pourtant cruciale pour localiser précisément les objets.

Dans les CNN classiques, on empile des couches de convolution qui extraient des caractéristiques de plus en plus abstraites, et on réduit progressivement la résolution spatiale à l’aide du *pooling*. La profondeur de l’image (le nombre de canaux) augmente, mais sa taille spatiale (hauteur × largeur) diminue. Finalement, les données sont aplaties et envoyées dans une ou plusieurs couches entièrement connectées pour produire une prédiction globale. Mais cette architecture n’est pas compatible avec une prédiction localisée pixel à pixel.

L’idée clé des FCN est de supprimer ces couches entièrement connectées et l’étape d’aplatissement, et de les remplacer par des convolutions $1 \times 1$. Cette opération permet de transformer chaque vecteur de caractéristiques (par pixel) en un score de classe, tout en conservant la structure spatiale. On obtient alors, en sortie, une carte par classe, où chaque pixel contient une prédiction de probabilité.

Un problème subsiste toutefois : à cause des opérations de *pooling* successives, la sortie du réseau est plus petite que l’image d’entrée. Or, pour produire une prédiction par pixel à la résolution d’origine, il est nécessaire de restaurer la taille initiale. Pour cela, @long2015fullyconvolutionalnetworkssemantic proposent l’utilisation de la convolution transposée (souvent appelée déconvolution, bien que ce terme soit techniquement incorrect), qui agit comme une opération inverse de la convolution en augmentant la résolution spatiale. La @fig-deconvolution illustre schématiquement ce mécanisme.

<!-- J'ai pas capté comment ca marche la déconvolution, ca assigne la valeur d'un pixel aux pixels voisins créés ? -->

::: {.center}
![Représentation de la déconvolution](../img/transposed_conv.png){#fig-deconvolution}
:::


<!-- The transpose of convolving a 3 × 3 kernel over a 4 × 4 input using unit strides (i.e., i = 4, k = 3, s = 1 and p = 0). It is equivalent to convolving a 3 × 3 kernel over a 2 × 2 input padded with a 2 × 2 border of zeros using unit strides (i.e., i' = 2, k' = k, s'= 1 and p' = 2). 
SOURCE : https://arxiv.org/pdf/1603.07285 Page 23-->


Pour résumer, la plupart des modèles de segmentation adoptent une architecture en U, composée de deux parties complémentaires :

- La branche descendante (encodeur), qui extrait progressivement des représentations de plus en plus abstraites de l’image. Elle applique des couches convolutives et des opérations de *pooling* successives afin de réduire la résolution tout en enrichissant la représentation sémantique. Le résultat est un embedding (vecteur de caractéristiques) condensé, moins volumineux que l’image initiale. Cette partie est similaire aux CNN.
- La branche montante (décodeur), qui reconstruit une carte de sortie à la même résolution que l’image d’entrée, en utilisant des opérations d’*upsampling* comme la convolution transposée. Cette partie vise à propager l’information sémantique vers les pixels, tout en rétablissant progressivement la structure spatiale.

Il est important de noter que les deux branches sont apprises : les poids des filtres convolutifs et transposés sont ajustés durant l'entraînement, via rétropropagation, comme dans n’importe quel réseau de neurones profond.

Parmi les extensions les plus remarquables aux FCN, on peut citer le modèle U-Net proposé par @Ronneberger_Fischer_Brox_2015b. Dans les architectures FCN classiques, les couches de convolution couplée au *pooling* induisent une perte progressive de détails spatiaux fins, ce qui tend à produire des contours flous dans les cartes de segmentation.
L'idée novatrice des auteurs réside dans sa structure symétrique en U, où chaque niveau de la partie montante est connecté à son équivalent dans la partie descendante via des connexions appelées *skip connections*. Ces connexions permettent au décodeur de récupérer directement les cartes de caractéristiques locales extraites aux étapes de l'encodeur, riches en détails de bas niveau (textures, contours), tout en conservant le contexte global appris dans les couches profondes. La @fig-unet illustre cette mécanique de fusion multi-niveaux.

Une autre avancée majeure est le modèle DeepLabv3, introduit par @chen2017rethinking. Ce dernier cherche également à atténuer la perte d’information due à l'opération de *pooling*, mais en adoptant une approche différente : l’utilisation de convolutions dilatées (*atrous convolutions*). Le principe est d’étendre la taille du noyau de convolution en insérant des espaces (zéros) entre les éléments du filtre. Cela permet d’augmenter le champ réceptif (vision globale du réseau) sans augmenter le nombre de paramètres à estimer.

::: {.center}
![Représentation schématique du U-Net](../img/Unet-remanie.png){#fig-unet}
:::

Malgré les avancées considérables apportées par les architectures convolutionnelles, ces dernières présentent plusieurs limitations structurelles. Par nature, une convolution opère sur un voisinage local (défini par la taille du noyau), ce qui restreint la capacité du réseau à capturer des dépendances à longue distance dans l’image. Pour élargir le champ de vision du modèle, il faut empiler plusieurs couches convolutives, ce qui augmente la profondeur du réseau… et donc son coût computationnel.

Par conséquent, si les CNN sont très efficaces pour modéliser les détails locaux, ils peinent à capturer des relations spatiales globales. Par exemple, deux parties d’un même objet spatialement éloignées (comme les ailes d’un avion ou les extrémités d’une route) sont rarement mises en relation par un CNN standard. De plus, les réseaux convolutionnels ne sont pas naturellement invariants au positionnement global dans l’image : une voiture située en haut à gauche ou en bas à droite peut être traitée différemment, bien qu’il s’agisse du même objet.

La révolution amorcée par les Transformers dans le traitement du langage naturel (NLP), à travers l'article fondateur de @vaswani2017attention, a rapidement essaimé vers d'autres domaines, dont celui de la vision par ordinateur. En 2020, les Vision Transformers (ViT), introduits par @dosovitskiy2021image, chercheurs chez Google, ont bouleversé les approches traditionnelles de la vision artificielle. Leur particularité réside dans l’adoption du même mécanisme fondamental que celui utilisé en NLP : le *self-attention*. Ce mécanisme permet au modèle de se concentrer sur différentes parties d’une image (comme il le ferait avec des mots dans une phrase) en les pondérant dynamiquement par leurs importances, afin de mieux en comprendre la structure et le contenu.

Contrairement aux CNN qui traitent directement l’image comme une grille de pixels 2D, les Vision Transformers commencent par diviser l’image en petits blocs réguliers, appelés *patches*. Chaque *patch* est ensuite aplati et encodé en vecteur (*embedding*), exactement comme un mot l’est dans un modèle de langage. Il faut donc s'imaginer qu'une image devient une séquence de vecteurs de la même manière qu'une phrase est une séquence de mots. Cependant, contrairement aux mots dans une phrase, les *patches* d’image n’ont pas d’ordre explicite ou structure syntaxique. Pour préserver la structure spatiale, on ajoute à chaque embedding un encodage positionnel (*positional encoding*), qui fournit au modèle une information sur la position d’origine du *patch* dans l’image.

Finalement, une fois tous les *patches* encodés, ils sont passés dans le mécanisme de self-attention, qui permet à chaque *patch* de s’informer de tous les autres *patches*, en attribuant à chacun une pondération calculée dynamiquement selon leur pertinence. Ce mécanisme permet de capturer à la fois des dépendances locales (entre *patches* voisins) et globales (entre régions éloignées), ce qui est particulièrement utile pour des objets de grande taille ou des structures étendues. 

Ce traitement est généralement répété à travers plusieurs couches de Transformer (notées "$L \times$" dans la @fig-vit), permettant un enrichissement progressif des représentations à chaque niveau. À la sortie, chaque *patch* est représenté par un vecteur qui incorpore à la fois son propre contenu et son contexte global.

On obtient alors de nouveaux vecteurs qui sont des représentations des *patches* individuels, enrichis par le contexte global. Il est important de noter qu'on empile généralement plusieurs couches Transformer (par exemple, "$L \times$" signifie L couches dans la @fig-vit). Dans la configuration originale du ViT <!-- ecrire Vision Transformer (ViT) // On l'a déjà mis au dessus non ? -->pour la classification d’image, un token spécial est ajouté à la séquence dès l’entrée. Après passage dans les couches Transformer, le vecteur final associé à ce token est utilisé comme représentation globale de l’image, que l’on transmet à une couche entièrement connectée pour effectuer la classification.

<!--Note:  Pour comprendre intuitivement le mecanisme d'attention, chaque split de l'image est un vecteur de grande dimension au départ. Ensuite on le compare aux vecteur de tous les autres split (patches) de l'image et on ajuste les poids du vecteur. On fait ca pour tous les splits et plusieurs fois en changeant la taille du vecteur pour chaque patch puis on utile tout les vecteurs pour de la classif ou segmentation -->

::: {.center}
![Représentation schématique du Vision Transformer (ViT)](../img/vit_schema.png){#fig-vit}
:::

Le succès des Vision Transformers (ViT), initialement conçus pour des tâches de classification d’images, a rapidement suscité un intérêt croissant pour leur adaptation à des tâches plus complexes, notamment la segmentation sémantique. Cependant, l’utilisation directe des ViT pour la segmentation se heurte à plusieurs limitations :

1. Les ViT standards ne conservent pas la structure spatiale de manière aussi précise que les CNN, et l’encodage positionnel appris se révèle souvent insuffisant pour localiser finement les objets au niveau du pixel.
2. Contrairement aux CNN qui construisent des représentations à différentes échelles (petits détails et vue globale), le ViT standard traite tous les *patches* à la même résolution.
3. Le mécanisme d’attention a un coût quadratique avec la taille des *patches*. Plus on veut de détails (donc des *patches* plus petits), plus ça devient lourd à calculer.

Pour répondre à ces défis, le modèle SegFormer, proposé par @xie2021segformer chez NVIDIA, introduit une approche hybride qui combine intelligemment les forces des CNN (efficacité locale et structure hiérarchique) avec celles des Transformers (apprentissage du contexte global via self-attention). Le modèle SegFormer, est défini par deux composantes principales (cf. @fig-segformer) un encodeur (hiérarchique) basé sur des couches Transformer empilées inspiré des CNN et un décodeur très simple pour produire la carte de segmentation. 

Contrairement au ViT standard qui applique le même traitement à tous les patches, SegFormer adopte une structure à plusieurs niveaux de résolution, analogue à celle des CNN. L’image est traitée par une succession de blocs Transformer, où chaque niveau réduit progressivement la résolution spatiale, construisant ainsi une hiérarchie d’abstractions.

De plus, le partitionnement de l’image se fait en *patches* chevauchants (*Overlapping Patch Merging*). Contrairement aux ViT où les *patches* sont non recouvrants (et donc spatialement disjoints), ici chaque *patch* inclut une portion de ses voisins, ce qui permet de préserver la continuité spatiale. Grâce à cette conception, et à la structure hiérarchique à plusieurs résolutions, il n’est plus nécessaire d’utiliser un encodage positionnel explicite : la position est captée implicitement par le recouvrement et la profondeur du traitement.

Finalement, l'une des particularités du Segformer est également son décodeur très simple qui contraste avec les décodeurs complexes de l'U-Net ou du DeepLab. En effet, il prend les sorties des différents niveaux - 4 dans le papier - de l'encodeur qui représentent des résolutions différentes. Il les projette dans un espace commun, les interpole à la même taille, puis les concatène pour produire la carte de segmentation via quelques couches simples. Cette conception permet au SegFormer de rester relativement léger en nombre de paramètres, tout en offrant des performances compétitives. En conséquence, le modèle est rapide à entraîner, efficace à l’inférence, et bien adapté aux contraintes de déploiement en production.

::: {.center}
![Représentation schématique du Segformer](../img/segformer_architecture.png){#fig-segformer}
:::

Outre le SegFormer, d'autres modèles de segmentation récents peuvent être considérés comme l’état de l’art. Par exemple, on peut citer SETR, proposé par @zheng2021SETR, qui fut l’un des premiers à adapter une architecture Transformer pure à la segmentation sémantique. Le Swin Transformer, introduit par @liu2021swin, repose quant à lui sur une approche hiérarchique utilisant des fenêtres glissantes à déplacement progressif (*shifted windows*), permettant de mieux capter les structures à différentes échelles tout en conservant une efficacité computationnelle élevée. Enfin, le Segment Anything Model v2 (SAM-2), récemment développé par Meta (@ravi2024sam2segmentimages), représente une avancée majeure dans la segmentation universelle. Il permet de segmenter automatiquement n’importe quel objet dans une image, à partir d’un simple point d’indication ou d’un prompt, sans entraînement spécifique à un domaine donné.

Dans le cadre de notre projet, après avoir évalué plusieurs architectures, notamment DeepLabv3, notre choix s’est porté sur le modèle SegFormer-B5[^segformer], avec lequel nous avons obtenu les meilleurs résultats en termes de précision de segmentation, tout en maintenant des temps de calcul raisonnables. Nous avons utilisé la version pré-entraînée du modèle proposée par NVIDIA, que nous avons ensuite spécialisée via un apprentissage supervisé sur nos images satellites des DROM. Ce transfert d’apprentissage nous a permis de bénéficier à la fois des connaissances générales acquises sur de grands jeux de données, et d’une adaptation fine aux caractéristiques particulières de la géographie des DROM.


[^segformer]: https://github.com/NVlabs/SegFormer


## Données

### Images satellites

Une image satellite est une matrice à trois dimensions. Chaque élément de cette image est un **pixel**, qui correspond à une surface au sol (par exemple 10m\*10m). Le pixel contient plusieurs valeurs numériques. Ces valeurs expriment l'intensité du rayonnement solaire reflété dans chaque **bande spectrale** pour ce pixel. Une bande spectrale correspond à une portion du spectre électromagnétique, qui peut être par exemple le bleu, le rouge, le vert, le proche infrarouge. Donc, pour une image satellite de n\*m pixels avec les trois bandes du visible (rouge, vert, bleu), chaque pixel aura trois valeurs différentes représentant l'intensité dans chacune des bandes du visible. Ainsi, un pixel qui représente 10m\*10m au sol donnera une couleur.

![Exemple d'un pixel d'une image satellite](../img/rgb.png){#fig-pixel width=30%}

Il existe de nombreux produits satellitaires disponibles. Nous nous sommes surtout concentrés sur deux d'entre eux : **Pléiades et Sentinel2**.

![Pléiades (gauche) vs Sentinel2 (droite) à Mamoudzou, Mayotte (2024)](../img/pleiades_vs_sentinel2.png){#fig-pleiadesvssentinel width=80%}

Les images satellites **Pléiades** constituent une ressource précieuse dans notre cas d’usage. Ce sont des images à très haute résolution, spécifiquement conçues pour l’observation fine des territoires. Elles offrent trois bandes spectrales dans le visible (**rouge, vert, bleu**) auxquelles s’ajoute une quatrième bande dans le proche infrarouge (NIR), bien que cette dernière ne soit pas disponible dans nos données actuelles. Leur résolution spatiale remarquable de **0,5m** par pixel permet une détection très précise des objets au sol, ce qui est essentiel pour nos traitements d’analyse fine.

Ces images peuvent être obtenues soit via les **archives gratuites** (sous conditions d’accord), soit par acquisition à la demande, un service payant encadré par une licence Airbus©, avec des délais d’environ 6 à 8 mois par département.

Dans le contexte du cyclone Chido, qui a frappé Mayotte en décembre 2024, un plan d’urgence a permis l’accès gratuit à une mosaïque d’images Pléiades post-cyclone couvrant l’île. Une **mosaïque** est une image composite constituée d’assemblages de prises de vues réalisées à différentes dates. Elle permet d’obtenir une couverture complète du territoire, avec un minimum de zones nuageuses et une meilleure homogénéité visuelle. Ce travail de reconstitution, mené par **l'Institut National de l'information géographique et forestière** (IGN), est crucial pour garantir des données exploitables, notamment dans le cadre de l’entraînement de modèles d’analyse automatique. Pour mener à bien le projet données satellites, un accord a été signé avec l'IGN afin de fournir à l'Insee des mosaïques archivées des DROM.

Mais Pléiades n’est pas notre seule source d’imagerie satellite. Les satellites **Sentinel-2**, développés par l’Agence spatiale européenne (ESA) dans le cadre du programme Copernicus, offrent une alternative open source, particulièrement intéressante pour les analyses à large échelle ou à forte fréquence temporelle. Contrairement à Pléiades, Sentinel-2 capte **treize bandes spectrales**, réparties entre le visible, le proche infrarouge (NIR) et l’infrarouge à ondes courtes (SWIR), ce qui ouvre la voie à une multitude d’indices et d’analyses thématiques (comme la détection de la végétation, de l’humidité, ou des matériaux).

En revanche, la résolution spatiale de Sentinel-2 est moindre : selon les bandes, elle varie entre **10 m**, 20 m et 60 m, ce qui rend l'observation de petits objets ou de détails fins plus difficile. Néanmoins, ces images ont l’avantage d’être acquises automatiquement **tous les cinq jours** et **gratuites**, garantissant une fréquence de revisite élevée et une mise à disposition régulière des données, y compris en période de crise.

Ainsi, les images Pléiades semblent être les plus adaptées pour de la détection de bâtiment dans les DROM. Les mosaïques disponibles par territoire et millésimes sont décrites [@fig-mosaiques]. 
<!-- ![Mosaïques d’images disponibles par DROM et par année] -->


::: {#fig-mosaiques .cell-output-display}
|                      | 2017 | 2018 | 2019 | 2020 | 2021 | 2022 | 2023 | 2024 | 2025 |
|:---------------------|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
| Guadeloupe           |      |  x   |  x   |  x   |      |  x   |      |      |      |
| Martinique           |      |      |      |      |      |  x   |      |      |      |
| Guyane               |      |      |      |      |      |  x   |  x   |  x   |      |
| La Réunion           |      |  x   |      |      |      |  x   |  x   |      |      |
| Mayotte              |  x   |  x   |  x   |  x   |  x   |  x   |  x   |  x   |  x   |
| Bonus : Saint Martin |      |      |      |      |      |      |      |  x   |      |
:::

### Annotations

Afin d'entraîner un modèle de segmentation sémantique, les images doivent être annotées pour que le modèle apprenne afin de reproduire, sur de nouvelles images, la même méthode de segmentation. Le coût d’une annotation à la main des bâtiments sur l'ensemble des images satellites utilisées pour l'entraînement étant prohibitif, il est donc nécessaire d'explorer des sources permettant d'automatiser cette tâche. ll faut toutefois noter, que sans labellisation manuelle des bâtiments au sein des images Pléiades utilisées pour l'entraînement, un décalage entre les images et les annotations existe. Ceci altère la qualité de l'entraînement.  

La **BDTOPO** est une base de type géométrique produite par l'IGN pour l'ensemble du territoire français. Celle-ci liste, entre autres, l’ensemble des constructions sous forme de polygones sur un territoire donné.  

Une autre source a été explorée : le **Répertoire des Immeubles Localisés (RIL)**, produite par l'Insee. Il est donc possible de produire des zones tampons autour des points localisant les bâtiments et de s’en servir comme une approximation pour labelliser les logements. Cette base de données n’englobe donc pas tous les bâtiments, mais uniquement ceux qui sont habités. Dans les DROM, les données sont en quantité limitée, puisque chaque année seulement un cinquième de la base est mis à jour, c'est-à-dire que les données peuvent dater d'il y a plus de quatre ans. Si la qualité du RIL est satisfaisante en France hexagonale, elle est loin d’être fiable dans les territoires d'outre-mer. Cette source ne permet pas de faire des labellisations adaptées pour l'entraînement et a été écartée.

Les deux sources précédentes offrent des annotations binaires : bâtiment oui/non. Il est également possible d'utiliser des annotations multiclasses. La base **COSIA**, générée par IA à partir des photographies aériennes de l’IGN à une résolution de 20 cm, offre des polygones multiclasses précis, disponibles également pour les DROM. Parmi ces classes, la classe bâtiment est incluse, ce qui constitue un avantage pour les performances du modèle. C’est cette source qui a été retenue pour l’entraînement. La @fig-millesimescosia informe de la couverture du sol COSIA disponible par millésime et par territoire. 

::: {#fig-millesimescosia .cell-output-display}
|                      | 2017 | 2018 | 2019 | 2020 | 2021 | 2022 | 2023 | 2024 | 2025 |
|:---------------------|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
| Guadeloupe           |  x   |      |      |      |      |  x   |      |  x   |      |
| Martinique           |  x   |      |      |      |      |  x   |      |      |      |
| Guyane               |      |  x   |      |      |  x   |      |  x   |      |      |
| La Réunion           |  x   |      |      |      |      |  x   |      |      |      |
| Mayotte              |      |      |      |      |      |      |  x   |      |      |
:::


<!-- mettre les images + les millesimes dispo de cosia -->