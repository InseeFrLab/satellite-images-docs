@article{zheng2022change,
    author = {Zhuo Zheng and others},
    title = {Change Is Everywhere: Single-Temporal Supervised Object Change Detection in Remote Sensing Imagery},
    journal = {arXiv},
    month = {August},
    year = {2022},
    eprint = {2108.07002},
    url = {http://arxiv.org/abs/2108.07002},
    note = {Accessed on April 4, 2023},
}

@techreport{berova2023,
    author = {Raya Berova},
    title = {{Détection automatisée de changements des bâtiments en France d'Outre-Mer à partir d'images satellites}},
    institution = {Direction Régionale de l'Insee Martinique, Direction Générale de l'Insee},
    year = {2023},
    type = {Rapport de stage},
    url = {https://minio.lab.sspcloud.fr/cguillo/rapport_stage/Rapport_de_stage_3A-2.pdf}
}

@article{bendjoudi_coefficient_2002,
	title = {Le coefficient de compacité de {Gravelius}: analyse critique d'un indice de forme des bassins versants},
	volume = {47},
	issn = {0262-6667, 2150-3435},
	shorttitle = {Le coefficient de compacité de {Gravelius}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02626660209493000},
	doi = {10.1080/02626660209493000},
	language = {fr},
	number = {6},
	urldate = {2024-05-07},
	journal = {Hydrological Sciences Journal},
	author = {Bendjoudi, Hocine and Hubert, Pierre},
	month = dec,
	year = {2002},
	pages = {921--930},
	file = {Bendjoudi et Hubert - 2002 - Le coefficient de compacité de Gravelius analyse .pdf:C\:\\Users\\RK09OA\\Zotero\\storage\\7SRJ23RR\\Bendjoudi et Hubert - 2002 - Le coefficient de compacité de Gravelius analyse .pdf:application/pdf},
}

@techreport{nabec2023,
    author = {Judith Nabec},
    title = {{Mise en place d'une méthode de détection automatique des évolutions de bâti en Outre-Mer sur des images satellites}},
    institution = {Direction Générale de l'Insee},
    year = {2023},
    type = {Rapport de stage},
    url = {https://minio.lab.sspcloud.fr/cguillo/rapport_stage/Rapport_de_stage_2023-3.pdf}
}

@techreport{chabennet2021,
    author = {Quentin Chabennet},
    title = {{Détection automatique de la couverture des sols à partir d’images satellites à l’aide de méthodes de segmentation sémantique}},
    institution = {ECE engineering school},
    year = {2021},
    type = {Mémoire de stage},
    url = {https://minio.lab.sspcloud.fr/cguillo/rapport_stage/rapport_stage_quentin_dmrg-1.pdf}
}


@article{xie2021segformer,
  title={SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers},
  author={Xie, Enze and Wang, Wenhai and Yuille, Alan L. and Anandkumar, Anima and Alvarez, Jose M.},
  journal={arXiv preprint arXiv:2105.15203},
  year={2021},
  month={October},
  url={http://arxiv.org/abs/2105.15203}
}

@article{chen2017rethinking,
  title={Rethinking Atrous Convolution for Semantic Image Segmentation},
  author={Chen, Liang-Chieh and et al.},
  journal={arXiv},
  month={December},
  year={2017},
  eprint={1706.05587},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  accessed={April 25, 2023},
  url={http://arxiv.org/abs/1706.05587}
}



@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017},
  organization={Curran Associates, Inc.},
  url={https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}
@article{dosovitskiy2021image,
  title={An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2021},
  month={June},
  url={http://arxiv.org/abs/2010.11929}
}


 % à utiliser pour parler de la segmentation sémantique

 @article{Garcia-Garcia_Orts-Escolano_Oprea_Villena-Martinez_Garcia-Rodriguez_2017, title={A Review on Deep Learning Techniques Applied to Semantic Segmentation}, url={http://arxiv.org/abs/1704.06857}, abstractNote={Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.}, note={arXiv: 1704.06857}, journal={arXiv:1704.06857 [cs]}, author={Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Garcia-Rodriguez, Jose}, year={2017}, month={Apr} }
 
 @article{Long_Shelhamer_Darrell_2015, title={Fully Convolutional Networks for Semantic Segmentation}, url={http://arxiv.org/abs/1411.4038}, abstractNote={Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.}, note={arXiv: 1411.4038}, journal={arXiv:1411.4038 [cs]}, author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor}, year={2015}, month={Mar} }

 @article{Ronneberger_Fischer_Brox_2015b, title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, url={http://arxiv.org/abs/1505.04597}, abstractNote={There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .}, note={arXiv: 1505.04597}, journal={arXiv:1505.04597 [cs]}, author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas}, year={2015}, month={May} }
 @article{Wagner_Dalagnol_Tarabalka_Segantine_Thomé_Hirye_2020, title={U-Net-Id, an Instance Segmentation Model for Building Extraction from Satellite Images—Case Study in the Joanópolis City, Brazil}, volume={12}, DOI={10.3390/rs12101544}, abstractNote={Currently, there exists a growing demand for individual building mapping in regions of rapid urban growth in less-developed countries. Most existing methods can segment buildings but cannot discriminate adjacent buildings. Here, we present a new convolutional neural network architecture (CNN) called U-net-id that performs building instance segmentation. The proposed network is trained with WorldView-3 satellite RGB images (0.3 m) and three different labeled masks. The first is the building mask; the second is the border mask, which is the border of the building segment with 4 pixels added outside and 3 pixels inside; and the third is the inner segment mask, which is the segment of the building diminished by 2 pixels. The architecture consists of three parallel paths, one for each mask, all starting with a U-net model. To accurately capture the overlap between the masks, all activation layers of the U-nets are copied and concatenated on each path and sent to two additional convolutional layers before the output activation layers. The method was tested with a dataset of 7563 manually delineated individual buildings of the city of Joanópolis-SP, Brazil. On this dataset, the semantic segmentation showed an overall accuracy of 97.67\% and an F1-Score of 0.937 and the building individual instance segmentation showed good performance with a mean intersection over union (IoU) of 0.582 (median IoU = 0.694).}, number={1010}, journal={Remote Sensing}, publisher={Multidisciplinary Digital Publishing Institute}, author={Wagner, Fabien H. and Dalagnol, Ricardo and Tarabalka, Yuliya and Segantine, Tassiana Y. F. and Thomé, Rogério and Hirye, Mayumi C. M.}, year={2020}, month={Jan}, pages={1544} }

 